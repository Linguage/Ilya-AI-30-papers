# 1409.2329v5.Recurrent_Neural_Network_Regularization.pdf

## 基本信息
- **标题**: Recurrent Neural Network Regularization
- **作者**: Wojciech Zaremba, Ilya Sutskever, Oriol Vinyals
- **机构**: New York University, Google Brain
- **会议**: ICLR 2015
- **arXiv版本**: v5 (2015年2月19日)
- **页数**: 8页

## 摘要

本文提出了一种针对具有长短期记忆（LSTM）单元的循环神经网络（RNN）的简单正则化技术。Dropout是正则化神经网络最成功的技术，但在RNN和LSTM中效果不佳。在本文中，我们展示了如何正确地将dropout应用于LSTM，并证明它大大减少了各种任务上的过拟合。这些任务包括语言建模、语音识别、图像字幕生成和机器翻译。

## 引言

### RNN的重要性
- **序列建模**: RNN是在语言建模、语音识别和机器翻译等重要任务上实现最先进性能的神经网络序列模型
- **正则化需求**: 神经网络的成功应用需要良好的正则化
- **当前问题**: Dropout在RNN中效果不佳，导致实际应用常使用过小的模型

### Dropout在RNN中的挑战
- **噪声放大**: Bayer等人声称传统的dropout在RNN中效果不佳，因为循环会放大噪声，从而损害学习
- **现有方法**: 现有的正则化方法对RNN的改进相对较小

### 本文贡献
- **正确应用**: 展示了如何正确地将dropout应用于RNN的特定连接子集
- **广泛验证**: 在三个不同问题上展示了强大的实证结果
- **代码开源**: 工作代码可在GitHub上找到

## 相关工作

### Dropout的发展
- **原始dropout**: Srivastava (2013)引入的正则化方法，在前馈神经网络中非常成功
- **扩展研究**: 许多工作以各种方式扩展了dropout
- **RNN应用**: 在RNN上应用dropout的研究相对较少

### RNN架构变体
- **长程依赖**: Hochreiter & Schmidhuber (1997), Graves et al. (2009), Cho et al. (2014)等人在处理长程依赖问题上表现更好
- **LSTM**: 最常用的RNN变体，本文专注于LSTM的正则化

### 应用任务
- **语言建模**: RNN首次取得实质性成功的任务
- **语音识别**: Robinson et al. (1996), Graves et al. (2013)
- **机器翻译**: 用于语言建模、重新排序或短语建模

## 技术方法

### 深度LSTM
- **表示法**: 下标表示时间步，上标表示层
- **状态维度**: 所有状态都是n维的
- **仿射变换**: Tn,m: Rn → Rm 表示仿射变换
- **预测输出**: 使用顶层激活hL_t来预测y_t

### LSTM单元
- **记忆机制**: LSTM具有复杂的动态，能够轻松"记忆"长时间步的信息
- **记忆细胞**: 长期记忆存储在记忆细胞向量c^l_t ∈ R^n中
- **决策能力**: LSTM可以决定覆盖记忆细胞、检索记忆细胞或为下一个时间步保留记忆

### 正则化方案
- **连接选择**: 将dropout应用于RNN连接的特定子集
- **噪声控制**: 避免循环放大噪声的问题
- **通用性**: 这种应用dropout的方式很可能也适用于其他RNN架构

## 实验验证

### 任务范围
1. **语言建模**
2. **语音识别**
3. **机器翻译**

### 实际影响
- **模型容量**: 允许使用更大的RNN模型而不会过拟合
- **性能提升**: 在各种任务上显著减少过拟合
- **实用价值**: 提高了RNN在实际应用中的效果

## 独立工作

### 并行研究
- **Pham等人**: 独立开发了完全相同的RNN正则化方法并应用于手写识别
- **重新发现**: 作者重新发现了这种方法并在广泛的问题上展示了强大的实证结果

### 其他相关工作
- **Pachitariu & Sahani**: 其他将dropout应用于LSTM的工作

## 意义与影响

1. **技术突破**: 解决了dropout在RNN中的应用问题
2. **实际应用**: 使得更大的RNN模型成为可能
3. **开源贡献**: 代码公开，促进社区发展
4. **广泛验证**: 在多个任务上验证了方法的有效性

这篇论文为RNN的正则化提供了重要的解决方案，极大地促进了深度学习在序列建模任务中的应用。