# 1410.5401v2.Neural_Turing_Machines.pdf

## 基本信息
- **标题**: Neural Turing Machines
- **作者**: Alex Graves, Greg Wayne, Ivo Danihelka
- **机构**: Google DeepMind, London, UK
- **arXiv版本**: v2 (2014年12月10日)
- **页数**: 26页

## 摘要

这篇论文通过将神经网络与外部内存资源耦合来扩展神经网络的能力，这些内存资源可以通过注意力过程进行交互。组合系统类似于图灵机或冯·诺依曼架构，但可以端到端微分，允许使用梯度下降有效训练。

### 核心贡献
- **架构创新**: 将神经网络与外部内存结合
- **可微分计算**: 整个系统可端到端微分
- **算法学习**: 能够从输入输出示例中推断简单算法

### 实验结果
- **算法学习**: 成功学习复制、排序和关联回忆等简单算法
- **端到端训练**: 使用梯度下降有效训练整个系统

## 引言要点

### 计算机程序的基本机制
1. **基本操作**: 如算术运算
2. **逻辑流控制**: 分支控制
3. **外部内存**: 计算过程中可以读写

### 现代机器学习的局限
- **成功领域**: 在复杂数据建模方面取得广泛成功
- **忽视机制**: 大部分忽略了逻辑流控制和外部内存的使用

### 循环神经网络的优势
- **时序建模**: 能够学习和执行长时间的数据复杂转换
- **图灵完备**: 理论上具有模拟任意过程的能力
- **实际困难**: 原理上可行，实践中不一定简单

### 神经图灵机的设计
- **内存增强**: 通过大型可寻址内存丰富标准循环网络的能力
- **类比图灵**: 类似于图灵用无限内存带增强有限状态机
- **可微分**: 与图灵机不同，NTM是可微分计算机

## 认知科学联系

### 工作记忆的相似性
- **认知过程**: 与人类认知中的"工作记忆"过程最相似
- **短期存储**: 信息短期存储和基于规则的操作能力
- **快速创建变量**: 解决需要将近似规则应用于"快速创建变量"的任务

### 注意力机制
- **选择性读写**: 使用注意力过程选择性地读写内存
- **学习使用**: 与大多数工作记忆模型不同，架构可以学习使用工作记忆

## 研究背景

### 心理学和神经科学
- **工作记忆理论**: 心理学中用于解释涉及信息短期操作的任务
- **中央执行器**: "中央执行器"集中注意力并对内存缓冲区中的数据执行操作
- **容量限制**: 工作记忆容量限制的研究

### 神经科学基础
- **前额叶皮层**: 工作记忆过程归因于前额叶皮层和基底神经节组成的系统
- **延迟期活动**: 在延迟期间单个神经元的持续放电或复杂的神经动力学
- **维度测量**: 基于群体代码"维度"测量的复杂上下文相关任务

## 技术意义

1. **架构创新**: 首次将神经网络与可寻址内存结合
2. **算法学习**: 能够学习简单的算法程序
3. **认知建模**: 为工作记忆提供了计算模型
4. **可微分计算**: 开创了可微分计算的新方向