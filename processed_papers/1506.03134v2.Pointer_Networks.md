# 1506.03134v2.Pointer_Networks.pdf

## 基本信息
- **标题**: Pointer Networks
- **作者**: Oriol Vinyals, Meire Fortunato, Navdeep Jaitly
- **机构**: Google Brain, UC Berkeley
- **arXiv版本**: v2 (2017年1月2日)
- **页数**: 9页

## 摘要

我们引入了一种新的神经架构来学习输出序列的条件概率，其中输出序列的元素是对应于输入序列位置的离散标记。这类问题不能通过现有的方法如序列到序列（seq2seq）和神经图灵机来简单解决，因为输出中每一步的目标类别数量取决于输入的长度，而输入长度是可变的。诸如排序可变大小序列和各种组合优化问题都属于这一类。我们的模型使用最近提出的神经注意力机制解决了可变大小输出字典的问题。它与之前的注意力尝试不同，因为它不是使用注意力来混合编码器的隐藏单元到上下文向量，而是使用注意力作为指针来选择输入序列的成员作为输出。我们称这种架构为指针网络（Ptr-Net）。

## 引言

### RNN的发展
- **历史背景**: 循环神经网络（RNN）已经被用于从示例中学习序列函数超过三十年
- **架构限制**: 传统架构限制在输入和输出以固定帧率可用的设置
- **序列到序列**: 最近引入的序列到序列范式通过使用一个RNN将输入序列映射到嵌入，另一个RNN将嵌入映射到输出序列来消除这些约束

### 注意力机制
- **内容基础**: Bahdanau等人通过使用基于内容的注意力机制从输入传播额外的上下文信息来增强解码器
- **应用领域**: 这些发展使得RNN可以应用于新领域，在自然语言处理的核心问题上取得最先进的结果

### 现有方法的局限
- **固定字典大小**: 这些方法仍然要求输出字典的大小事先固定
- **组合问题**: 由于这个限制，我们不能直接将这个框架应用于输出字典大小取决于输入序列长度的组合问题

### 指针网络的创新
- **注意力重用**: 通过重新利用注意力机制创建指向输入元素的指针来解决这个限制
- **变长字典**: 结果架构（Ptr-Nets）可以训练输出三个组合优化问题的满意解决方案
- **纯数据驱动**: 产生的模型以纯数据驱动的方式产生这些问题的近似解决方案

## 主要贡献

1. **新架构**: 提出了简单有效的新架构——指针网络
2. **变长字典**: 使用softmax概率分布作为"指针"来处理表示变长字典的基本问题
3. **几何问题应用**: 将指针网络模型应用于三个涉及几何的不同非平凡算法问题
4. **泛化能力**: 显示学习模型可以泛化到比训练问题更多点的测试问题
5. **TSP求解器**: 指针网络模型学习了一个竞争性的小规模（n ≤ 50）TSP近似求解器

## 技术方法

### 模型对比
- **序列到序列模型**: 使用RNN将输入序列映射到代码向量，然后使用另一个RNN生成输出序列
- **指针网络**: 编码RNN将输入序列转换为代码，生成网络产生一个向量来调节基于内容的注意力机制

### 注意力机制的创新应用
- **传统注意力**: 用于混合编码器的隐藏单元到上下文向量
- **指针注意力**: 作为指针选择输入序列的成员作为输出
- **字典大小**: 注意力机制的输出是softmax分布，字典大小等于输入长度

## 应用任务

### 三个几何问题
1. **平面凸包**: 找到点的平面凸包
2. **Delaunay三角剖分**: 计算Delaunay三角剖分
3. **旅行商问题**: 对称平面旅行商问题（TSP）

### 实验结果
- **泛化能力**: 学习的模型可以泛化到超过训练最大长度的测试问题
- **近似求解**: 对于计算上棘手的问题，纯数据驱动方法可以学习近似解决方案
- **竞争性能**: 在小规模TSP问题上学习到竞争性的近似求解器

## 技术意义

### 算法学习
- **神经学习**: 鼓励对离散问题的神经学习进行更广泛的探索
- **数据驱动**: 证明了纯数据驱动方法可以解决复杂的组合优化问题
- **泛化性**: 模型能够泛化到训练期间未见过的更大规模问题

### 架构创新
- **指针机制**: 创新性地使用注意力机制作为指针
- **变长输出**: 解决了变长输出字典的根本问题
- **简单有效**: 架构简单但效果显著

这篇论文为解决组合优化问题提供了新的神经网络方法，展示了深度学习在传统算法领域的潜力。