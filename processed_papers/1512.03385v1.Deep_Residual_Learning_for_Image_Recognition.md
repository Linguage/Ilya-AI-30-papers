# 1512.03385v1.Deep_Residual_Learning_for_Image_Recognition.pdf

## 基本信息
- **标题**: Deep Residual Learning for Image Recognition
- **作者**: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun
- **机构**: Microsoft Research
- **arXiv版本**: v1 (2015年12月10日)
- **页数**: 12页

## 摘要

这篇论文提出了残差学习框架，用于训练比以往使用的网络深度显著增加的网络。作者明确地将层重构为学习相对于层输入的残差函数，而不是学习无参考函数。

### 核心问题
- **深度网络训练困难**: 更深的神经网络更难训练
- **退化问题**: 当网络深度增加时，准确率趋于饱和甚至下降

### 解决方案
- **残差学习**: 让层学习残差函数而非原始映射
- **深度优势**: 可以从显著增加的深度中获得准确性提升

### 实验结果
- **ImageNet**: 评估了高达152层的残差网络，比VGG网络深8倍但复杂度更低
- **集成模型**: 在ImageNet测试集上实现3.57%的错误率
- **竞赛成绩**: 赢得ILSVRC 2015分类任务第一名
- **CIFAR-10**: 使用100层和1000层网络进行分析

## 引言要点

### 深度网络的重要性
- **表示深度**: 对于许多视觉识别任务，表示的深度至关重要
- **实际成果**: 仅凭极深的表示，就在COCO目标检测数据集上获得了28%的相对改进

### 核心问题
1. **梯度消失/爆炸**: 已经通过归一化初始化和中间归一化层解决
2. **退化问题**: 当深度网络开始收敛时暴露的问题
   - 深度网络训练误差更高
   - 不是由过拟合引起的

### 残差学习框架
- **公式化**: 将层重构为学习残差函数
- **优势**: 残差网络更容易优化
- **应用**: 成为ILSVRC & COCO 2015竞赛提交的基础

## 技术贡献

### 残差块设计
- **跳跃连接**: 允许信息直接流过网络
- **恒等映射**: 当不需要额外信息时，网络可以学习恒等映射
- **深度扩展**: 支持构建非常深的网络架构

### 实验验证
- **ImageNet**: 证明了深度网络的优势
- **CIFAR-10**: 验证了极深网络的可行性
- **目标检测**: 在检测任务上的成功应用

## 影响与意义

1. **突破性进展**: 解决了深度网络训练的退化问题
2. **广泛应用**: 残差网络成为深度学习的基础架构
3. **竞赛成功**: 在多个计算机视觉竞赛中取得优异成绩
4. **后续影响**: 为更深的网络架构铺平了道路，影响了深度学习的发展方向