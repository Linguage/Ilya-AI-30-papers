# 1706.03762v7.Attention_Is_All_You_Need.pdf

## 基本信息
- **标题**: Attention Is All You Need
- **作者**: Ashish Vaswani, Llion Jones, Noam Shazeer, Niki Parmar, Aidan N. Gomez, Jakob Uszkoreit, Łukasz Kaiser, Illia Polosukhin
- **机构**: Google Brain, Google Research, University of Toronto
- **会议**: NIPS 2017 (NeurIPS 2017)
- **页数**: 15页
- **arXiv版本**: v7 (2023年8月2日)

## 摘要

这篇论文提出了Transformer架构，这是一个完全基于注意力机制的新网络架构，完全摒弃了循环和卷积结构。

### 核心贡献
1. **全新架构**: 提出了基于纯注意力机制的Transformer模型
2. **性能优越**: 在两个机器翻译任务上表现优异
3. **训练效率**: 更好的并行性，训练时间显著减少
4. **泛化能力**: 成功应用于英语成分句法分析

### 实验结果
- **WMT 2014英德翻译**: 28.4 BLEU，比现有最佳结果提高2 BLEU
- **WMT 2014英法翻译**: 单模型41.8 BLEU，创纪录
- **训练时间**: 8个GPU上仅需3.5天训练

## 引言要点

### 背景问题
- **循环神经网络的局限性**: 
  - RNN、LSTM、GRU已成为序列建模的标准方法
  - 计算沿着符号位置分解，本质上是顺序的
  - 无法在训练样本内并行化，内存限制影响批处理

### 注意力机制的发展
- 注意力机制已成为序列建模和转换模型的重要组成部分
- 允许建模依赖关系而不考虑其在输入或输出序列中的距离
- 但大多数情况下仍与循环网络结合使用

### Transformer的创新
- **摒弃循环**: 完全依赖注意力机制来绘制输入和输出之间的全局依赖关系
- **高度并行**: 允许更显著的并行化
- **快速训练**: 在8个P100 GPU上仅训练12小时即可达到新的翻译质量标准

## 技术背景

### 相关工作
论文还提到了减少顺序计算的其他相关工作，如Extended Neural GPU等，但Transformer是第一个完全基于注意力机制的架构。

## 影响与意义

这篇论文是深度学习领域的里程碑式工作：
1. **开创性**: 首次证明可以完全摒弃循环结构
2. **实用性**: 在实际任务中取得了突破性成果
3. **影响力**: 成为后续大语言模型的基础架构
4. **扩展性**: 架构具有良好的扩展性，支持大规模应用