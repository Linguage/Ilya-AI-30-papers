# 1806.01822v2.Relational_recurrent_neural_networks.pdf

## 基本信息
- **标题**: Relational Recurrent Neural Networks
- **作者**: Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Théophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, Timothy Lillicrap
- **机构**: DeepMind, University College London
- **arXiv版本**: v2 (2018年6月28日)
- **页数**: 18页

## 摘要

基于记忆的神经网络通过利用长期记忆信息的能力来建模时间数据。然而，目前尚不清楚它们是否也能够用所记忆的信息执行复杂的关系推理。在这里，我们首先证实了我们的直觉，即标准记忆架构可能在那些大量涉及理解实体连接方式的任务上挣扎——即涉及关系推理的任务。然后，我们通过使用一个新的记忆模块——关系记忆核心（RMC）——来改进这些缺陷，该模块采用多头点积注意力来允许记忆之间相互作用。最后，我们在一系列可能从更强大的跨序列信息关系推理中受益的任务上测试了RMC，并在强化学习领域（如Mini PacMan）、程序评估和语言建模方面显示出巨大收益，在WikiText-103、Project Gutenberg和GigaWord数据集上实现了最先进的结果。

## 引言

### 人类记忆与神经网络记忆
- **人类记忆系统**: 人类使用复杂的记忆系统来访问和推理重要信息，而不管这些信息最初是什么时候被感知的
- **神经网络记忆**: 在神经网络研究中，许多建模序列数据的成功方法也使用记忆系统，如LSTM和记忆增强神经网络

### 记忆交互的重要性
- **现有局限**: 虽然当前模型可以学习隔离和关联分布式、向量化的记忆，但它们并不偏向于显式地这样做
- **核心假设**: 这样的偏向可能使模型更好地理解记忆之间的关联，从而可能使其具有更好的跨时间关系推理能力

### 关系记忆核心（RMC）
- **创新设计**: 允许记忆之间相互作用的多头点积注意力机制
- **架构优势**: 结合了LSTM、记忆增强神经网络和非局部网络的构建块
- **应用领域**: 强化学习、程序评估、语言建模等多个任务

## 关系推理

### 关系推理的定义
- **核心概念**: 关系推理是理解实体之间连接方式并利用这种理解来完成更高层次目标的过程
- **实例说明**: 
  - 树木距离排序：需要比较和对比实体（树木和长椅）之间的关系（距离）
  - 单独推理无法达到解决方案

### 神经网络中的关系推理
- **卷积核**: 计算感受野内实体（像素）的关系（线性组合）
- **消息传递网络**: 节点构成实体，关系通过应用于连接节点的可学习函数计算
- **关系网络**: 利用输入图像中的空间局部性，专注于计算每对实体之间的二元关系
- **注意力机制**: 隐式执行某种形式的关系推理，通过内容而非邻近性来更好地关联嵌入

## 模型架构

### 关系记忆核心设计
- **指导原则**: 提供架构骨干，使模型能够学习隔离信息，并学习计算隔离信息之间的相互作用
- **构建块**: 结合LSTM、记忆增强神经网络和Transformer的组件
- **记忆槽**: 考虑固定数量的记忆槽，但允许记忆槽之间通过注意力机制相互作用

### 多头点积注意力
- **基本操作**: 每个记忆将参与所有其他记忆，并根据参与信息更新其内容
- **计算过程**: 
  1. 线性投影构建查询、键和值
  2. 缩放点积注意力计算权重
  3. 加权平均更新记忆内容
- **多头机制**: 允许记忆使用每个头向不同目标共享不同信息

### 记忆编码与递归
- **新信息编码**: 通过修改注意力公式高效地将新信息整合到记忆矩阵中
- **LSTM集成**: 将更新嵌入到LSTM中，实现递归处理
- **门控机制**: 类似LSTM的门控，支持记忆的更新和输出

## 实验任务

### 监督学习任务
1. **Nth Farthest任务**: 设计用于测试跨时间关系推理能力
   - 输入：随机采样向量序列
   - 目标：回答"第n远的向量离向量m有多远"的问题
   - 挑战：必须计算与参考向量m的所有成对距离关系，然后隐式排序

2. **程序评估**: Learning to Execute数据集
   - 包含图灵完备编程语言的算法片段
   - 测试模型对符号操作的理解能力
   - 评估记忆任务和操作指令执行

### 强化学习任务
- **Mini Pacman with viewport**: 部分可观察环境
  - 智能体必须预测鬼魂动态并规划导航
  - 通过视口获取信息，需要记忆整合

### 语言建模
- **数据集**: WikiText-103、Project Gutenberg、GigaWord v5
- **任务**: 建模条件概率p(wt|w<t)
- **应用**: 预测键盘、搜索短语完成、机器翻译、语音识别、信息检索

## 实验结果

### Nth Farthest任务
- **显著差异**: LSTM和DNC基线模型无法超过30%的最佳批次准确率，而RMC始终达到91%
- **扩展性**: 当使用32维向量增加任务难度时，RMC仍能保持性能
- **注意力分析**: 揭示了RMC内部功能的重要特征

### 程序评估
- **性能优越**: RMC在所有任务上至少与所有基线一样好
- **训练方式**: 无教师强制训练取得更好结果
- **泛化能力**: 放松真实值要求改善了模型泛化

### Mini-Pacman
- **性能提升**: RMC比LSTM获得约100分（677 vs 550）
- **完整观察**: 使用完整观察训练时，RMC性能几乎是LSTM的两倍（1159 vs 598）

### 语言建模
- **一致性改进**: 在所有三个语言建模任务中都观察到更低的困惑度
- **相对改进**: 比最佳发表结果降低1.4-5.4困惑度，相当于5-12%的相对改进
- **数据效率**: 模型学习的数据效率略优于LSTM
- **上下文利用**: 在评估期间提供的上下文词相对较少时得分较高

## 技术贡献

### 方法创新
1. **关系记忆核心**: 提出了允许记忆相互作用的新的记忆模块
2. **注意力机制**: 使用多头点积注意力实现记忆间交互
3. **架构整合**: 成功结合了多种神经网络架构的优势

### 实证验证
- **多领域成功**: 在监督学习、强化学习和语言建模等多个领域取得成功
- **性能突破**: 在多个基准测试上实现最先进结果
- **关系推理**: 证明了显式建模记忆交互的有效性

### 理论意义
- **记忆交互**: 强调了记忆交互在关系推理中的重要性
- **架构设计**: 展示了如何通过架构设计嵌入特定的推理能力
- **模型理解**: 为理解神经网络中的关系推理提供了新视角

## 意义与影响

1. **技术突破**: 解决了神经网络在关系推理方面的挑战
2. **方法创新**: 提出了简单而有效的关系记忆架构
3. **广泛应用**: 在多个不同领域展示了强大的适用性
4. **研究启发**: 为进一步研究记忆机制和关系推理提供了重要方向

这篇论文为关系推理和记忆系统提供了重要的神经网络解决方案，展示了如何通过专门设计的架构来解决传统深度学习方法难以处理的关系推理问题。