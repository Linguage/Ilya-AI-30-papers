# colt93-Keeping_Neural_Networks_Simple_by_Minimizing_the_Description_Length_of_the_Weights.pdf

## 基本信息
- **标题**: Keeping Neural Networks Simple by Minimizing the Description Length of the Weights
- **作者**: Geoffrey E. Hinton, Drew van Camp
- **机构**: University of Toronto
- **会议**: COLT 1993
- **页数**: 9页

## 摘要

监督神经网络如果权重中的信息远少于训练用例输出向量中的信息，那么它们会很好地泛化。因此在学习过程中，通过惩罚权重包含的信息量来保持权重简单是很重要的。权重中的信息量可以通过添加高斯噪声来控制，并且噪声水平可以在学习过程中调整，以优化网络的期望平方误差与权重信息量之间的权衡。我们描述了一种计算包含非线性隐藏单元层的网络中噪声权重的期望平方误差和信息量的导数的方法。如果输出单元是线性的，可以高效计算精确导数，而无需耗时的蒙特卡洛模拟。最小化描述神经网络权重所需信息量的想法导致了多种有趣的权重编码方案。

## 引言

### 过拟合问题
- **数据稀缺**: 在许多实际学习任务中，可用的训练数据很少，任何相当复杂的模型都会倾向于过拟合
- **信息平衡**: 为避免过拟合，需要确保权重中的信息少于训练用例输出向量中的信息
- **现有方法**: 研究人员已经考虑了多种限制权重信息的方法，包括限制连接数、权重共享、权重量化等

### 最小描述长度原则
- **核心思想**: 最佳模型是最小化描述模型和描述模型与数据失配的组合成本
- **神经网络应用**: 对于预定架构的监督神经网络，模型成本是描述权重所需的比特数
- **数据失配成本**: 描述网络输出与正确输出之间差异所需的比特数

## 最小描述长度原则的应用

### 通信模型
- **发送者**: 可以看到输入向量和正确输出
- **接收者**: 只能看到输入向量
- **通信过程**: 发送者首先拟合神经网络，然后发送权重，最后发送每个训练用例的失配

### 数据失配编码
- **量化假设**: 假设数据失配被精细量化，使用固定宽度的间隔
- **高斯分布**: 假设数据失配从零均值高斯分布中绘制
- **描述长度**: 最小化描述长度等同于最小化常用的平方误差函数

## 权重编码方法

### 简单权重编码
- **高斯假设**: 假设训练网络的权重被精细量化并来自零均值高斯分布
- **权重衰减**: 如果高斯分布的标准差预先固定，权重的描述长度与其平方和成正比
- **MDL解释**: 权重衰减改善泛化可以看作是这种粗略MDL方法的验证

### 混合高斯模型
- **改进方案**: 使用多个高斯混合分布来更准确地建模训练网络中的权重分布
- **自适应调整**: 混合高斯的均值、方差和混合比例在网络训练过程中自适应调整
- **聚类效果**: 权重被吸引到附近聚类中心，实现更有效的编码

## 噪声权重

### 信息控制方法
- **高斯噪声**: 添加零均值高斯噪声是限制数字信息量的标准方法
- **分布优化**: 从权重空间中的特定点开始，改变权重向量云的均值和方差以降低成本函数
- **权衡关系**: 高方差权重通信成本更低，但会导致数据失配的额外方差

### 权重描述长度
- **先验分布**: 发送者和接收者就给定权重的高斯先验分布达成一致
- **后验分布**: 学习后，发送者有权重的高斯后验分布
- **KL散度**: 通信后验分布所需的比特数等于从先验到后验的不对称散度

### "比特返还"论证
- **通信过程**: 发送者首先使用随机位源将后验分布折叠为精确值
- **权重发送**: 使用先验高斯分布编码精确权重
- **失配发送**: 发送使用这些权重实现的数据失配
- **信息恢复**: 接收者可以恢复发送者用来折叠分布的所有随机位

## 数据失配的期望描述长度

### 精确计算方法
- **线性输出单元**: 如果只有一个隐藏层且输出单元是线性的，可以精确计算期望平方误差
- **高斯噪声**: 权重假设具有独立的高斯噪声
- **表格计算**: 使用表格计算隐藏单元输出的均值和方差
- **反向传播**: 可以反向传播精确导数，但需要另一个表格来允许导数通过隐藏单元反向传播

## 让数据决定先验

### 自适应先验分布
- **问题**: 如果预先固定编码先验的均值和方差，可能选择不合适的值
- **解决方案**: 允许编码先验的均值和方差在优化过程中确定
- **超先验**: 可以用指定编码先验均值和方差概率分布的超先验来解释

### 混合高斯先验
- **灵活性**: 单个高斯先验不足以捕获权重中的某些常见结构
- **混合模型**: 使用自适应高斯混合分布来建模权重分布
- **聚类效果**: 权重被吸引到附近聚类中心，实现更有效的编码

## 混合高斯编码方案

### 编码过程
1. **高斯选择**: 从混合分布中随机选择一个高斯分布
2. **选择通信**: 将选择的高斯通信给接收者
3. **样本通信**: 使用选定的高斯分布通信样本值
4. **分布重建**: 接收者可以重建后验分布并恢复所有随机位

### 统计力学引理
- **自由能**: 期望能量减去熵
- **玻尔兹曼分布**: 最小化自由能的概率分布
- **对应关系**: 将混合中的每个高斯与物理系统的替代状态等同起来

## 实现与结果

### 实现细节
- **共轭梯度法**: 使用共轭梯度法同时优化所有参数
- **表格大小**: 使用300×300表格进行线性插值
- **语义检查**: 通过改变每个参数的小步长来验证实现的正确性

### 实验结果
- **任务**: 预测一类肽分子的有效性
- **数据**: 105个训练用例，420个测试用例
- **网络结构**: 4个隐藏单元，521个自适应权重
- **性能**: 相对误差0.286，显著优于无信息惩罚的0.967

### 对比结果
- **简单权重衰减**: 最佳相对误差0.317
- **噪声权重方法**: 相对误差0.286
- **权重聚类**: 权重形成三个相当尖锐的聚类

## 技术贡献

### 理论创新
1. **MDL原则应用**: 将最小描述长度原则系统地应用于神经网络训练
2. **噪声权重**: 提出了通过噪声权重控制信息量的方法
3. **精确计算**: 为线性输出单元网络提供了精确的期望误差计算方法

### 实践创新
1. **混合高斯编码**: 使用自适应混合高斯分布实现灵活的权重编码
2. **比特返还**: 创新的信息通信和恢复机制
3. **实现技术**: 提供了具体的实现方法和验证技术

### 实证验证
- **实验验证**: 在实际任务上验证了方法的有效性
- **性能对比**: 与传统方法进行了性能对比
- **聚类效果**: 展示了权重聚类的实际效果

## 意义与影响

1. **理论基础**: 为神经网络正则化提供了信息论基础
2. **方法创新**: 提出了新的权重复杂度控制方法
3. **实践指导**: 为实际应用中的正则化参数选择提供了指导
4. **研究方向**: 为后续研究开辟了新的方向

这篇论文为神经网络正则化和泛化提供了重要的理论基础，展示了如何通过信息论原理来理解和改进神经网络的泛化性能。Hinton和van Camp的工作为后续的贝叶斯神经网络、变分推断等领域的发展奠定了重要基础。